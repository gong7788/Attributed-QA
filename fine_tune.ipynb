{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from torch.utils.data import DataLoader\n",
    "from dataloader import doc2dialDataset\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('data/doc2dial/Train_dataset/DEFAULT/DEFAULT_withRefs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[:10].to_csv('data/doc2dial/TEST/DDP_Finetune.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data/doc2dial/Train_dataset/DEFAULT/DEFAULT_withRefs.csv'\n",
    "\n",
    "dataset = doc2dialDataset(path)\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = b['question']\n",
    "answers = b['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello, I forgot o update my address, can you help me with that?',\n",
       " 'Can I do my DMV transactions online?',\n",
       " 'Thanks, and in case I forget to bring all of the documentation needed to the DMV office, what can I do?',\n",
       " 'Ok, and can you tell me again where should I report my new address?']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.distributed as dist\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.multiprocessing as mp\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import os\n",
    "from QAModel import format_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.distributed import DistributedSampler\n",
    "\n",
    "def prepare(rank, world_size, batch_size=2, pin_memory=False, num_workers=0):\n",
    "    dataset = doc2dialDataset(path)\n",
    "    sampler = DistributedSampler(dataset, num_replicas=world_size, rank=rank, shuffle=False, drop_last=False)\n",
    "    \n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, pin_memory=pin_memory, num_workers=num_workers, drop_last=False, shuffle=False, sampler=sampler)\n",
    "    \n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gpus = torch.cuda.device_count()\n",
    "world_size = n_gpus\n",
    "DDP_dataloader = prepare(rank=0, world_size=2, batch_size=2, pin_memory=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_batch = next(iter(DDP_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello, I forgot o update my address, can you help me with that?',\n",
       " 'Thanks, and in case I forget to bring all of the documentation needed to the DMV office, what can I do?']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qs = first_batch['question']\n",
    "qs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup(rank, world_size):\n",
    "    os.environ['MASTER_ADDR'] = 'localhost'\n",
    "    os.environ['MASTER_PORT'] = '12355'\n",
    "\n",
    "    # initialize the process group\n",
    "    dist.init_process_group(\"gloo\", rank=rank, world_size=world_size)\n",
    "\n",
    "def cleanup():\n",
    "    dist.destroy_process_group()\n",
    "\n",
    "\n",
    "def demo_parallel(rank, world_size):\n",
    "    print(f\"Running DDP with model parallel example on rank {rank}.\")\n",
    "    setup(rank, world_size)\n",
    "\n",
    "    DDP_dataloader = prepare(rank, world_size, batch_size=2, pin_memory=False, num_workers=0)\n",
    "    first_batch = next(iter(DDP_dataloader))\n",
    "\n",
    "    tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "    model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "\n",
    "    model = model.to(rank)\n",
    "    ddp_model = DDP(model, device_ids=[rank])\n",
    "\n",
    "    for epoch in epochs:\n",
    "        #encoding inputs\n",
    "        dataloader.sampler.set_epoch(epoch)\n",
    "\n",
    "        for step, x in enumerate(epoch):\n",
    "\n",
    "            qs = first_batch['question']\n",
    "            answers = first_batch['answer']\n",
    "            refs = first_batch['ref'] # ?\n",
    "\n",
    "            temps = format_prompt(qs, refs)\n",
    "            \n",
    "            encoding = tokenizer(temps, \n",
    "                                return_tensors=\"pt\", \n",
    "                                padding='longest', \n",
    "                                max_length=1024, \n",
    "                                truncation=True)\n",
    "            \n",
    "            input_ids, attention_mask = encoding.input_ids, encoding.attention_mask\n",
    "            #encoding targets\n",
    "            target_encoding = tokenizer(answers,\n",
    "                                        return_tensors=\"pt\",\n",
    "                                        padding='longest',\n",
    "                                        max_length=1024,\n",
    "                                        truncation=True)\n",
    "            labels = target_encoding.input_ids\n",
    "\n",
    "            loss_fn = nn.MSELoss()\n",
    "            optimizer = optim.SGD(ddp_model.parameters(), lr=0.001)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = ddp_model(input_ids, attention_mask)\n",
    "\n",
    "            loss_fn(outputs, labels).backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    cleanup()\n",
    "\n",
    "def run_demo(demo_fn, world_size):\n",
    "    mp.spawn(demo_fn,\n",
    "             args=(world_size,),\n",
    "             nprocs=world_size,\n",
    "             join=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/home/ygong/miniconda3/envs/atlas-env/lib/python3.8/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/home/ygong/miniconda3/envs/atlas-env/lib/python3.8/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'demo_parallel' on <module '__main__' (built-in)>\n"
     ]
    },
    {
     "ename": "ProcessExitedException",
     "evalue": "process 1 terminated with exit code 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mProcessExitedException\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39massert\u001b[39;00m n_gpus \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m2\u001b[39m, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mRequires at least 2 GPUs to run, but got \u001b[39m\u001b[39m{\u001b[39;00mn_gpus\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m world_size \u001b[39m=\u001b[39m n_gpus\n\u001b[0;32m----> 4\u001b[0m run_demo(demo_parallel, world_size)\n",
      "Cell \u001b[0;32mIn[33], line 65\u001b[0m, in \u001b[0;36mrun_demo\u001b[0;34m(demo_fn, world_size)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_demo\u001b[39m(demo_fn, world_size):\n\u001b[0;32m---> 65\u001b[0m     mp\u001b[39m.\u001b[39;49mspawn(demo_fn,\n\u001b[1;32m     66\u001b[0m              args\u001b[39m=\u001b[39;49m(world_size,),\n\u001b[1;32m     67\u001b[0m              nprocs\u001b[39m=\u001b[39;49mworld_size,\n\u001b[1;32m     68\u001b[0m              join\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/atlas-env/lib/python3.8/site-packages/torch/multiprocessing/spawn.py:240\u001b[0m, in \u001b[0;36mspawn\u001b[0;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[1;32m    236\u001b[0m     msg \u001b[39m=\u001b[39m (\u001b[39m'\u001b[39m\u001b[39mThis method only supports start_method=spawn (got: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m).\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[1;32m    237\u001b[0m            \u001b[39m'\u001b[39m\u001b[39mTo use a different start_method use:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[1;32m    238\u001b[0m            \u001b[39m'\u001b[39m\u001b[39m torch.multiprocessing.start_processes(...)\u001b[39m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m start_method)\n\u001b[1;32m    239\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(msg)\n\u001b[0;32m--> 240\u001b[0m \u001b[39mreturn\u001b[39;00m start_processes(fn, args, nprocs, join, daemon, start_method\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mspawn\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/atlas-env/lib/python3.8/site-packages/torch/multiprocessing/spawn.py:198\u001b[0m, in \u001b[0;36mstart_processes\u001b[0;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[39mreturn\u001b[39;00m context\n\u001b[1;32m    197\u001b[0m \u001b[39m# Loop on join until it returns True or raises an exception.\u001b[39;00m\n\u001b[0;32m--> 198\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m context\u001b[39m.\u001b[39;49mjoin():\n\u001b[1;32m    199\u001b[0m     \u001b[39mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/atlas-env/lib/python3.8/site-packages/torch/multiprocessing/spawn.py:149\u001b[0m, in \u001b[0;36mProcessContext.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[39mraise\u001b[39;00m ProcessExitedException(\n\u001b[1;32m    141\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mprocess \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m terminated with signal \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m    142\u001b[0m             (error_index, name),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    146\u001b[0m             signal_name\u001b[39m=\u001b[39mname\n\u001b[1;32m    147\u001b[0m         )\n\u001b[1;32m    148\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 149\u001b[0m         \u001b[39mraise\u001b[39;00m ProcessExitedException(\n\u001b[1;32m    150\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mprocess \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m terminated with exit code \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m    151\u001b[0m             (error_index, exitcode),\n\u001b[1;32m    152\u001b[0m             error_index\u001b[39m=\u001b[39merror_index,\n\u001b[1;32m    153\u001b[0m             error_pid\u001b[39m=\u001b[39mfailed_process\u001b[39m.\u001b[39mpid,\n\u001b[1;32m    154\u001b[0m             exit_code\u001b[39m=\u001b[39mexitcode\n\u001b[1;32m    155\u001b[0m         )\n\u001b[1;32m    157\u001b[0m original_trace \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39merror_queues[error_index]\u001b[39m.\u001b[39mget()\n\u001b[1;32m    158\u001b[0m msg \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m-- Process \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m terminated with the following error:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m error_index\n",
      "\u001b[0;31mProcessExitedException\u001b[0m: process 1 terminated with exit code 1"
     ]
    }
   ],
   "source": [
    "n_gpus = torch.cuda.device_count()\n",
    "assert n_gpus >= 2, f\"Requires at least 2 GPUs to run, but got {n_gpus}\"\n",
    "world_size = n_gpus\n",
    "run_demo(demo_parallel, world_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atlas-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
